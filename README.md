# Adaptive Database Framework

A self-adaptive database framework that autonomously ingests a live JSON data stream, infers field types, classifies every field into **MySQL** (structured) or **MongoDB** (semi-structured), and routes records to the appropriate backend â€” all without hardcoded schemas.


---

## ğŸ“š Project Overview

The system consumes health-tracker JSON records from a FastAPI data stream, normalises naming conventions, observes field patterns through statistical analysis, and dynamically decides which database backend each field belongs to. Linking fields (`username`, `sys_ingested_at`) are stored in **both** backends to enable cross-database joins.

### Key Capabilities

| Capability | Description |
|---|---|
| **Dynamic Schema Inference** | No predefined schemas â€” field types discovered from data |
| **Adaptive Placement** | Heuristic rules decide SQL vs MongoDB per field |
| **Cross-DB Linking** | `username` + `sys_ingested_at` in both backends for joins |
| **Metadata Persistence** | Classification decisions survive process restarts |
| **Bi-temporal Timestamps** | `t_stamp` (client) + `sys_ingested_at` (server) |

---

## ğŸ—ï¸ Architecture

The codebase is organized into **4 topics** + a final orchestrator:

```
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚  Data Stream API â”‚
                         â”‚  (FastAPI :8000) â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚  raw JSON records
                                  â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                    IngestAndClassify                       â”‚
 â”‚                                                            â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
 â”‚  â”‚ TOPIC 1 â€” NORMALIZATION              src/normalization/ â”‚
 â”‚  â”‚  FieldNormalizer Â· TypeDetector Â· RecordNormalizerâ”‚      â”‚
 â”‚  â”‚  â€¢ camelCase/PascalCase â†’ snake_case             â”‚      â”‚
 â”‚  â”‚  â€¢ Detect IP vs float, UUID, datetime            â”‚      â”‚
 â”‚  â”‚  â€¢ Inject sys_ingested_at timestamp              â”‚      â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
 â”‚                     â”‚ normalized records                    â”‚
 â”‚                     â–¼                                       â”‚
 â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                 â”‚
 â”‚               â”‚  BUFFER   â”‚  in-memory staging              â”‚
 â”‚               â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                                 â”‚
 â”‚                     â”‚ flush (size or timeout)               â”‚
 â”‚                     â–¼                                       â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
 â”‚  â”‚ TOPIC 2 â€” ANALYSIS & CLASSIFICATION  src/analysis/      â”‚
 â”‚  â”‚  FieldAnalyzer Â· FieldStats Â· Classifier         â”‚      â”‚
 â”‚  â”‚  â€¢ Track presence %, type stability, nesting     â”‚      â”‚
 â”‚  â”‚  â€¢ Apply heuristic rules â†’ PlacementDecision     â”‚      â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
 â”‚                     â”‚ decisions                             â”‚
 â”‚                     â–¼                                       â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
 â”‚  â”‚ TOPIC 3 â€” STORAGE                   src/storage/        â”‚
 â”‚  â”‚  MySQLClient Â· MongoClient Â· RecordRouter        â”‚      â”‚
 â”‚  â”‚  â€¢ Dynamic CREATE TABLE / ALTER TABLE            â”‚      â”‚
 â”‚  â”‚  â€¢ Split record â†’ SQL part + Mongo part          â”‚      â”‚
 â”‚  â”‚  â€¢ Batch insert into both backends               â”‚      â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
 â”‚                     â”‚                                       â”‚
 â”‚                     â–¼                                       â”‚
 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
 â”‚  â”‚ TOPIC 4 â€” PERSISTENCE               src/persistence/   â”‚
 â”‚  â”‚  MetadataStore                                   â”‚      â”‚
 â”‚  â”‚  â€¢ Save/load decisions, stats, mappings to JSON  â”‚      â”‚
 â”‚  â”‚  â€¢ Enables restart without re-analysis           â”‚      â”‚
 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Classification Rules

| # | Condition | â†’ Backend |
|---|---|---|
| 1 | Field is `username`, `sys_ingested_at`, or `t_stamp` | **BOTH** |
| 2 | Value is nested (dict / list) | **MongoDB** |
| 3 | Presence â‰¥ 70% AND type stability â‰¥ 90% | **SQL** |
| 4 | Everything else | **MongoDB** |

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.12+**
- **Docker & Docker Compose**
- **Poetry** (`pip install poetry`)

### 1 Â· Start databases

```bash
docker-compose up -d          # MySQL 8.0  +  MongoDB 7.0
```

### 2 Â· Install dependencies

```bash
poetry install
```

### 3 Â· Configure environment

```bash
cp .env.example .env
# Edit .env if you need non-default ports/passwords
```

### 4 Â· Run the data stream API (separate terminal)

```bash
git clone https://github.com/YogeshKMeena/Course_Resources.git
cd Course_Resources/CS432_Databases/Assignments/T2
pip install -r requirements.txt
uvicorn simulation_code:app --reload --port 8000
```

### 5 Â· Run the pipeline

```bash
# Ingest 100 records
poetry run python -m src.cli ingest --count 100

# Or run continuously
poetry run python -m src.cli ingest --continuous --interval 0.5

# Check status
poetry run python -m src.cli status

# View placement decisions
poetry run python -m src.cli decisions
```

---

## ï¿½ Programmatic Usage

### Using the Pipeline Class Directly

The `IngestAndClassify` class provides the complete pipeline orchestration. For convenience, a `StreamingPipeline` wrapper is also available.

#### Example 1: Basic Batch Processing

```python
from src.ingest_and_classify import IngestAndClassify

# Initialize pipeline (auto-loads config from .env)
pipeline = IngestAndClassify()

# Process records
records = [
    {"username": "alice", "age": 30, "city": "NYC"},
    {"username": "bob", "score": 95.5, "metadata": {"level": 5}}
]

# Ingest batch
pipeline.ingest_batch(records)

# Check status
status = pipeline.get_status()
print(f"Buffer size: {status['buffer_size']}")
print(f"Total processed: {status['total_records_processed']}")

# Get classification summary
summary = pipeline.get_classification_summary()
print(f"SQL fields: {summary['counts']['sql']}")
print(f"MongoDB fields: {summary['counts']['mongo']}")

# Close connections
pipeline.close()
```

#### Example 2: Streaming with Context Manager

```python
from src.pipeline import StreamingPipeline

# Use context manager for automatic cleanup
with StreamingPipeline() as pipeline:
    # Stream 100 records from the data source
    summary = pipeline.start_streaming(max_records=100)
    
    # Results are auto-flushed and connections closed
    print(f"Rate: {summary['records_per_second']} rec/sec")
```

#### Example 3: Manual Record-by-Record Processing

```python
from src.ingest_and_classify import IngestAndClassify

pipeline = IngestAndClassify()

# Process one record at a time
for i in range(100):
    record = fetch_from_somewhere()  # Your data source
    pipeline.ingest(record)
    
    # Manual flush when needed
    if i % 50 == 0:
        result = pipeline.flush()
        print(f"Flushed: {result['records_processed']} records")

# Get placement decisions
decisions = pipeline.get_decisions()
for field_name, decision in decisions.items():
    print(f"{field_name} â†’ {decision.backend.name} ({decision.reason})")

pipeline.close()
```

#### Example 4: Inspect Field Statistics

```python
from src.ingest_and_classify import IngestAndClassify

pipeline = IngestAndClassify()

# Process some data
pipeline.ingest_batch(your_records)

# Get detailed field statistics
field_stats = pipeline.get_field_stats()

for field_name, stats in field_stats.items():
    print(f"\nField: {field_name}")
    print(f"  Presence: {stats.presence_count} records")
    print(f"  Dominant type: {stats.dominant_type}")
    print(f"  Type stability: {stats.type_stability:.2%}")
    print(f"  Unique ratio: {stats.unique_ratio:.2%}")
    print(f"  Is nested: {stats.is_nested}")

pipeline.close()
```

#### Example 5: Using the Streaming Wrapper

```python
from src.pipeline import StreamingPipeline

# Create pipeline
pipeline = StreamingPipeline()

# Option 1: Stream from configured data source
pipeline.start_streaming(max_records=50, interval_seconds=0.1)

# Option 2: Process your own batch
my_records = [...]
result = pipeline.process_batch(my_records)

# Option 3: Process single records
pipeline.process_single({"username": "test", "value": 123})

# Check current status
status = pipeline.get_pipeline_status()
print(f"Will auto-flush: {status['will_auto_flush']}")

# Get field placement decisions
decisions = pipeline.get_field_decisions()

# Cleanup
pipeline.close()
```

#### Configuration Options

The pipeline uses configuration from `.env` or can be passed directly:

```python
from src.config import AppConfig, MySQLConfig, MongoConfig, BufferConfig
from src.ingest_and_classify import IngestAndClassify

# Custom configuration
config = AppConfig(
    mysql=MySQLConfig(host="localhost", port=3306, database="my_db"),
    mongo=MongoConfig(host="localhost", port=27017, database="my_db"),
    buffer=BufferConfig(buffer_size=100, buffer_timeout_seconds=10.0),
    data_stream_url="http://localhost:8000/GET/record",
    metadata_dir="./my_metadata/"
)

pipeline = IngestAndClassify(config)
```

#### CLI Wrapper

For quick testing, use the pipeline module directly:

```bash
# Run demo with sample data
poetry run python -m src.pipeline demo

# Stream from data source
poetry run python -m src.pipeline stream

# Stream specific number of records
poetry run python -m src.pipeline stream 100
```

---

## ï¿½ğŸ“ Project Structure

```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ normalization/               # Topic 1
â”‚   â”‚   â”œâ”€â”€ field_normalizer.py      #   FieldNormalizer  â€” name canonicalization
â”‚   â”‚   â”œâ”€â”€ type_detector.py         #   TypeDetector     â€” semantic type detection
â”‚   â”‚   â””â”€â”€ record_normalizer.py     #   RecordNormalizer â€” full record pipeline
â”‚   â”‚
â”‚   â”œâ”€â”€ analysis/                    # Topic 2
â”‚   â”‚   â”œâ”€â”€ field_stats.py           #   FieldStats       â€” per-field statistics
â”‚   â”‚   â”œâ”€â”€ field_analyzer.py        #   FieldAnalyzer    â€” observation engine
â”‚   â”‚   â”œâ”€â”€ decision.py              #   PlacementDecision, Backend enum, thresholds
â”‚   â”‚   â””â”€â”€ classifier.py            #   Classifier       â€” heuristic rules
â”‚   â”‚
â”‚   â”œâ”€â”€ storage/                     # Topic 3
â”‚   â”‚   â”œâ”€â”€ mysql_client.py          #   MySQLClient      â€” dynamic DDL + inserts
â”‚   â”‚   â”œâ”€â”€ mongo_client.py          #   MongoClient      â€” document inserts + indexes
â”‚   â”‚   â””â”€â”€ record_router.py         #   RecordRouter     â€” split & route records
â”‚   â”‚
â”‚   â”œâ”€â”€ persistence/                 # Topic 4
â”‚   â”‚   â””â”€â”€ metadata_store.py        #   MetadataStore    â€” JSON-based persistence
â”‚   â”‚
â”‚   â”œâ”€â”€ config.py                    # Configuration (env vars / .env)
â”‚   â”œâ”€â”€ ingest_and_classify.py       # â˜… IngestAndClassify orchestrator
â”‚   â””â”€â”€ cli.py                       # CLI entry point
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ conftest.py                  # Shared fixtures
â”‚   â””â”€â”€ test_ingest_and_classify.py  # Test skeleton
â”‚
â”œâ”€â”€ docker-compose.yml               # MySQL 8.0 + MongoDB 7.0
â”œâ”€â”€ pyproject.toml                   # Poetry config + ruff/mypy/pytest
â”œâ”€â”€ .env.example                     # Environment template
â”œâ”€â”€ .pre-commit-config.yaml          # Ruff + mypy hooks
â””â”€â”€ .github/
    â”œâ”€â”€ workflows/ci.yml             # Lint â†’ Test CI pipeline
    â””â”€â”€ PULL_REQUEST_TEMPLATE.md
```

---

## ğŸ§ª Testing

```bash
# All tests
poetry run pytest

# With coverage report
poetry run pytest --cov=src --cov-report=term-missing

# Specific topic
poetry run pytest tests/test_normalization.py
poetry run pytest tests/test_analysis.py
poetry run pytest tests/test_storage.py
poetry run pytest tests/test_persistence.py

# Integration tests only
poetry run pytest -m integration
```

---

## ğŸ”§ Development

### Code Quality

```bash
poetry run ruff format .          # Auto-format
poetry run ruff check . --fix     # Lint + auto-fix
poetry run mypy src/              # Type checking
```

### Pre-commit Hooks

```bash
poetry run pre-commit install     # One-time setup
poetry run pre-commit run --all-files   # Manual run
```

### Branching Convention

```
main                â† stable, passing CI
â”œâ”€â”€ topic1/â€¦        â† normalization work
â”œâ”€â”€ topic2/â€¦        â† analysis & classification work
â”œâ”€â”€ topic3/â€¦        â† storage work
â””â”€â”€ topic4/â€¦        â† persistence work
```

---



## ğŸ“– References

- [Course Project Document](./Databases_CS432__2026_%20Track%202.pdf)
- [Data Stream API (Course Repo)](https://github.com/YogeshKMeena/Course_Resources/tree/main/CS432_Databases/Assignments/T2)
- [API Endpoint](http://127.0.0.1:8000/GET/record/{count})


